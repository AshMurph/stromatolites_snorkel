{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Candidate Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['SNORKELDB']=\"postgres:///stromatolite\"\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the `Sentence` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import Sentence\n",
    "\n",
    "sentences = session.query(Sentence).all()\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a `Candidate` schema\n",
    "We now define the schema of the relation mention we want to extract (which is also the schema of the candidates).  This must be a subclass of `Candidate`, and we define it using a helper function.\n",
    "\n",
    "Here we'll define a binary _spouse relation mention_ which connects two `Span` objects of text.  Note that this function will create the table in the database backend if it does not exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "StromStrat = candidate_subclass('StromStrat', ['strom', 'stratname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a basic `CandidateExtractor`\n",
    "\n",
    "Next, we'll write a basic function to extract **candidate spouse relation mentions** from the corpus.  The `SentenceParser` we used in Part I is built on [CoreNLP](http://stanfordnlp.github.io/CoreNLP/), which performs _named entity recognition_ for us.\n",
    "\n",
    "We will extract `Candidate` objects of the `Spouse` type by identifying, for each `Sentence`, all pairs of ngrams (up to trigrams) that were tagged as people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a child context space for our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams\n",
    "\n",
    "ngram_strom = Ngrams(n_max=1)\n",
    "ngram_strat = Ngrams(n_max=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use a `PersonMatcher` to enforce that candidate relations are composed of pairs of spans that were tagged as people by the `SentenceParser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import RegexMatchSpan\n",
    "\n",
    "strom_matcher = RegexMatchSpan(rgx=\"stromatolit|thrombolit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.matchers import DictionaryMatch\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "request = urllib.urlopen('https://macrostrat.org/api/v2/defs/strat_names?all')\n",
    "data = json.loads(request.read())\n",
    "\n",
    "#FULL STRAT NAME\n",
    "strat_dict_long = { r['strat_name_long'] for r in data['success']['data'] }\n",
    "\n",
    "#ABBREVIATED STRAT NAME - V1\n",
    "strat_dict_abV1 = { r['strat_name'] + ' ' + r['rank'] for r in data['success']['data'] }\n",
    "\n",
    "#ABBREVIATED STRAT NAME - V2\n",
    "strat_dict_abV2 = { r['strat_name'] + ' ' + r['rank'] + '.' for r in data['success']['data'] }\n",
    "\n",
    "#LITHOLOGY STRAT NAMES\n",
    "request = urllib.urlopen('https://macrostrat.org/api/v2/defs/lithologies?all')\n",
    "lithologies = json.loads(request.read())\n",
    "lithologies=[l['name'].capitalize() for l in lithologies['success']['data']]\n",
    "\n",
    "strat_dict_short = { r['strat_name'] for r in data['success']['data'] }\n",
    "\n",
    "strat_dict_lith=set()\n",
    "for r in strat_dict_short:\n",
    "    if r.split(' ')[-1] in lithologies:\n",
    "        strat_dict_lith.add(r)\n",
    "        \n",
    "strat_dict=set(list(strat_dict_long)+list(strat_dict_abV1)+list(strat_dict_abV2)+list(strat_dict_lith))\n",
    "        \n",
    "strat_matcher=DictionaryMatch(d=strat_dict,ignore_case=False,longest_match_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we combine the candidate class, child context space, and matcher into an extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.candidates import CandidateExtractor\n",
    "\n",
    "ce = CandidateExtractor(StromStrat, [ngram_strom, ngram_strat], [strom_matcher, strat_matcher],\n",
    "                        symmetric_relations=True, nested_relations=False, self_relations=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the `CandidateExtractor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the `CandidateExtractor` by calling extract with the contexts to extract from, a name for the `CandidateSet` that will contain the results, and the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time c = ce.extract(sentences, 'Candidate Set', session)\n",
    "print \"Number of candidates:\", len(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the extracted candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session.add(c)\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train / test sets now...\n",
    "\n",
    "Splitting by _document_; first, let's see the distribution of candidates by document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "candidates_by_doc = defaultdict(set)\n",
    "for cand in c:\n",
    "    candidates_by_doc[cand[0].parent.document.id].add(cand)\n",
    "\n",
    "plt.hist(map(len, candidates_by_doc.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And total number of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(candidates_by_doc.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, split the candidates into train / test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "doc_ids = list(candidates_by_doc.keys())\n",
    "shuffle(doc_ids)\n",
    "split = int(0.66 * len(doc_ids))\n",
    "\n",
    "train = CandidateSet(name='Training Candidates')\n",
    "session.add(train)\n",
    "for doc_id in doc_ids[:split]:\n",
    "    for cand in candidates_by_doc[doc_id]:\n",
    "        train.append(cand)\n",
    "print len(train)\n",
    "\n",
    "test = CandidateSet(name='Test Candidates')\n",
    "session.add(test)\n",
    "for doc_id in doc_ids[split:]:\n",
    "    for cand in candidates_by_doc[doc_id]:\n",
    "        test.append(cand)\n",
    "print len(test_candidates)\n",
    "\n",
    "session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading the candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snorkel.models import CandidateSet\n",
    "\n",
    "train = session.query(CandidateSet).filter(CandidateSet.name == 'Training Candidates').one()\n",
    "print len(train)\n",
    "\n",
    "test = session.query(CandidateSet).filter(CandidateSet.name == 'Test Candidates').one()\n",
    "print len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `Viewer` to inspect candidates\n",
    "\n",
    "Next, we'll use the `Viewer` class--here, specifically, the `SentenceNgramViewer`--to inspect the data.\n",
    "\n",
    "It is important to note, our goal here is to **maximize the recall of true candidates** extracted, **not** to extract _only_ the correct candidates. Learning to distinguish true candidates from false candidates is covered in Tutorial 4.\n",
    "\n",
    "First, we instantiate the `Viewer` object, which groups the input `Candidate` objects by `Sentence`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "sv = SentenceNgramViewer(train, session)\n",
    "sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sv.get_selected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can **navigate using the provided buttons**, or **using the keyboard (hover over buttons to see controls)**, highlight candidates (even if they overlap), and also **apply binary labels** (more on where to use this later!).  In particular, note that **the Viewer is synced dynamically with the notebook**, so that we can for example get the `Candidate` that is currently selected. Try it out!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
